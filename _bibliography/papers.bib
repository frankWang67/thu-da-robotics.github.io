---
---
@article{yu2025robotic,
  arxiv   = {2502.07472},
  author  = {Yu, Mingrui and Jiang, Yongpeng and Chen, Chen and Jia, Yongyi and Li, Xiang},
  doi     = {10.1109/LRA.2025.3555138},
  journal = {IEEE Robotics and Automation Letters},
  number  = {5},
  pages   = {4738-4745},
  preview = {yu2025robotic.gif},
  title   = {Robotic In-Hand Manipulation for Large-Range Precise Object Movement: The RGMC Champion Solution},
  volume  = {10},
  bibtex_show   = {true},
  website = {https://rgmc-xl-team.github.io/ingrasp_manipulation/},
  year    = {2025},
}

@inproceedings{yan2024unified,
  archiveprefix = {arXiv},
  arxiv         = {2302.05685},
  author        = {Xiangjie Yan and Luo Shaqi and Yongpeng Jiang and Mingrui Yu and Chen Chen and Senqiang Zhu and Gao Huang and Shiji Song and Xiang Li},
  bibtex_show   = {true},
  booktitle     = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  eprint        = {2302.05685},
  preview       = {yan2023multimodal.png},
  primaryclass  = {cs.RO},
  title         = {A Unified Interaction Control Framework for Safe Robotic Ultrasound Scanning with Human-Intention-Aware Compliance},
  website       = {https://yanseim.github.io/iros24ultrasound/},
  year          = {2024},
}

@article{yu2023generalizable,
  arxiv       = {2310.09899},
  author      = {Yu, Mingrui and Lv, Kangchen and Wang, Changhao and Jiang, Yongpeng and Tomizuka, Masayoshi and Li, Xiang},
  bibtex_show = {true},
  journal     = {The International Journal of Robotics Research},
  preview     = {yu2023generalizable.gif},
  title       = {Generalizable whole-body global manipulation of deformable linear objects by dual-arm robot in 3-D constrained environments},
  website     = {https://mingrui-yu.github.io/DLO_planning_2/},
  year        = {2024},
}

@inproceedings{yu2024inhand,
  arxiv       = {2403.12676},
  author      = {Yu, Mingrui and Liang, Boyuan and Zhang, Xiang and Zhu, Xinghao and Li, Xiang and Tomizuka, Masayoshi},
  bibtex_show = {true},
  booktitle   = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  preview     = {yu2024inhand.gif},
  title       = {In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing},
  website     = {https://mingrui-yu.github.io/DLO_following/},
  year        = {2024},
}

@inproceedings{jiang2024contact,
  archiveprefix = {arXiv},
  arxiv         = {2402.18897},
  author        = {Yongpeng Jiang and Mingrui Yu and Xinghao Zhu and Masayoshi Tomizuka and Xiang Li},
  bibtex_show   = {true},
  booktitle     = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  eprint        = {2402.18897},
  preview       = {jiang2024contact.png},
  primaryclass  = {cs.RO},
  title         = {Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach},
  website       = {https://director-of-g.github.io/in_hand_manipulation/},
  year          = {2024},
}

@inproceedings{chen2024visual,
  archiveprefix = {arXiv},
  arxiv         = {2405.09359},
  author        = {Chen Chen and Qikai Zou and Yuhang Song and Shiji Song and Xiang Li},
  bibtex_show   = {true},
  booktitle     = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  eprint        = {2405.09359},
  preview       = {chen2024visual.png},
  primaryclass  = {cs.RO},
  title         = {Visual Attention Based Cognitive Human--Robot Collaboration for Pedicle Screw Placement in Robot-Assisted Orthopedic Surgery},
  year          = {2024},
}

@article{yan2024complementary,
  abstract    = {There is invariably a tradeoff between safety and efficiency for collaborative robots (cobots) in human–robot collaborations (HRCs). Robots that interact minimally with humans can work with high speed and accuracy but cannot adapt to new tasks or respond to unforeseen changes, whereas robots that work closely with humans can but only by becoming passive to humans, meaning that their main tasks are suspended and efficiency compromised. Accordingly, this article proposes a new complementary framework for HRC that balances the safety of humans and the efficiency of robots. In this framework, the robot carries out given tasks using a vision-based adaptive controller, and the human expert collaborates with the robot in the null space. Such a decoupling drives the robot to deal with existing issues in task space [e.g., uncalibrated camera, limited field of view (FOV)] and null space (e.g., joint limits) by itself while allowing the expert to adjust the configuration of the robot body to respond to unforeseen changes (e.g., sudden invasion, change in environment) without affecting the robot’s main task. In addition, the robot can simultaneously learn the expert’s demonstration in task space and null space beforehand with dynamic movement primitives (DMPs). Therefore, an expert’s knowledge and a robot’s capability are explored and complement each other. Human demonstration and involvement are enabled via a mixed interaction interface, i.e., augmented reality (AR) and haptic devices. The stability of the closed-loop system is rigorously proved with Lyapunov methods. Experimental results in various scenarios are presented to illustrate the performance of the proposed method.},
  author      = {Yan, Xiangjie and Jiang, Yongpeng and Chen, Chen and Gong, Leiliang and Ge, Ming and Zhang, Tao and Li, Xiang},
  bibtex_show = {true},
  doi         = {10.1109/TCST.2023.3301675},
  issn        = {1558-0865},
  journal     = {IEEE Transactions on Control Systems Technology},
  month       = {Jan},
  number      = {1},
  pages       = {112-127},
  preview     = {yan2024complementary.png},
  title       = {A Complementary Framework for Human–Robot Collaboration With a Mixed AR–Haptic Interface},
  volume      = {32},
  year        = {2024},
}

@inproceedings{jia2024efficient,
  abstract    = {Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments. Potential applications include drug delivery, diagnostics, and therapeutic interventions. Existing techniques commonly impart magnetic properties to the target object,or drive the robot to contact and then manipulate the object, both probably inducing physical damage. This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact. Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze. To deal with it, this paper proposes a data-driven-based solution. A neural network is constructed to efficiently estimate the motion model. Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints. Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment. Experimental results are presented to show the tracking and navigation performance of the proposed scheme.},
  arxiv       = {2403.14414},
  author      = {Jia, Yongyi and Miao, Shu and Zhou, Junjian and Jiao, Niandong and Liu, Lianqing and Li, Xiang},
  bibtex_show = {true},
  booktitle   = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  preview     = {jia2024efficient.png},
  title       = {Efficient Model Learning and Adaptive Tracking Control of Magnetic Micro-Robots for Non-Contact Manipulation},
  year        = {2024},
}

@inproceedings{chen2023safe,
  archiveprefix = {arXiv},
  arxiv         = {2309.08178},
  author        = {Chen, Yu and Chen, Gong and Ye, Jing and Qiu, Xiangjun and Li, Xiang},
  bibtex_show   = {true},
  booktitle     = {2024 International Conference on Robotics and Automation (ICRA)},
  eprint        = {2309.08178},
  month         = {May},
  preview       = {chensafe23.gif},
  primaryclass  = {cs.RO},
  title         = {Safe and Individualized Motion Planning for Upper-limb Exoskeleton Robots Using Human Demonstration and Interactive Learning},
  year          = {2024},
}


@article{chen2024learning,
  arxiv       = {2309.14720},
  author      = {Yu Chen and Shu Miao and Gong Chen and Jing Ye and Chenglong Fu and Bin Liang and Shiji Song and Xiang Li},
  bibtex_show = {true},
  journal     = {IEEE Transactions on Robotics},
  month       = {Sep},
  pages       = {4699-4718},
  preview     = {chen23HIL.gif},
  title       = {Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-in-the-Loop Adaptation Framework for Lower-Limb Exoskeleton},
  volume      = {40},
  year        = {2024},
}


@inproceedings{jiang2023contact,
  arxiv       = {2303.03635},
  author      = {Yongpeng Jiang and Yongyi Jia and Xiang Li},
  bibtex_show = {true},
  booktitle   = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi         = {10.1109/IROS55552.2023.10341476},
  number      = {},
  pages       = {10604-10611},
  preview     = {jiang2023contact.png},
  title       = {Contact-Aware Non-prehensile Robotic Manipulation for Object Retrieval in Cluttered Environments},
  volume      = {},
  website     = {https://director-of-g.github.io/push_in_clutter/},
  year        = {2023},
}

@inproceedings{shu2023two,
  author       = {Shu, Yana and Chen, Yu and Zhang, Xuan and Zhang, Shisheng and Chen, Gong and Ye, Jing and Li, Xiang},
  booktitle    = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi          = {10.1109/IROS55552.2023.10342056},
  organization = {IEEE},
  pages        = {2470--2476},
  preview      = {shu23two.png},
  title        = {Two-Stage Trajectory-Tracking Control of Cable-Driven Upper-Limb Exoskeleton Robots with Series Elastic Actuators: A Simple, Accurate, and Force-Sensorless Method},
  year         = {2023},
}

@article{yu2023global,
  arxiv       = {2205.04004},
  author      = {Yu, Mingrui and Lv, Kangchen and Zhong, Hanzhong and Song, Shiji and Li, Xiang},
  bibtex_show = {true},
  code        = {https://github.com/Mingrui-Yu/shape_control_DLO_2},
  doi         = {10.1109/TRO.2022.3200546},
  journal     = {IEEE Transactions on Robotics},
  number      = {1},
  pages       = {417-436},
  preview     = {yu2023global.gif},
  title       = {Global Model Learning for Large Deformation Control of Elastic Deformable Linear Objects: An Efficient and Adaptive Approach},
  volume      = {39},
  website     = {https://mingrui-yu.github.io/shape_control_DLO_2/},
  year        = {2023},
}

@inproceedings{yu2023acoarse,
  arxiv       = {2209.11145},
  author      = {Yu, Mingrui and Lv, Kangchen and Wang, Changhao and Tomizuka, Masayoshi and Li, Xiang},
  bibtex_show = {true},
  booktitle   = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  doi         = {10.1109/ICRA48891.2023.10160264},
  pages       = {10153-10159},
  preview     = {yu2023acoarse.gif},
  title       = {A Coarse-to-Fine Framework for Dual-Arm Manipulation of Deformable Linear Objects with Whole-Body Obstacle Avoidance},
  website     = {https://mingrui-yu.github.io/DLO_planning/},
  year        = {2023},
}

@inproceedings{lv2023learning,
  arxiv       = {2210.01433},
  author      = {Lv, Kangchen and Yu, Mingrui and Pu, Yifan and Jiang, Xin and Huang, Gao and Li, Xiang},
  bibtex_show = {true},
  booktitle   = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  doi         = {10.1109/ICRA48891.2023.10160784},
  keywords    = {Point cloud compression;Learning systems;Geometry;Solid modeling;Automation;Shape;Wires},
  number      = {},
  pages       = {7119-7125},
  preview     = {lv2023learning.gif},
  title       = {Learning to Estimate 3-D States of Deformable Linear Objects from Single-Frame Occluded Point Clouds},
  volume      = {},
  year        = {2023},
}

@inproceedings{zhang2023multi,
  author       = {Zhang, Xuan and Shu, Yana and Chen, Yu and Chen, Gong and Ye, Jing and Li, Xiu and Li, Xiang},
  booktitle    = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  doi          = {10.1109/ICRA48891.2023.10161255},
  organization = {IEEE},
  pages        = {10490--10496},
  preview      = {zhan23multi.png},
  title        = {Multi-Modal Learning and Relaxation of Physical Conflict for an Exoskeleton Robot with Proprioceptive Perception},
  year         = {2023},
}

@inproceedings{jia2022hierarchical,
  abstract     = {Laser-driven micro-tools are formulated by treating highly-focused laser beams as actuators, to control the tool's motion to contact then manipulate a micro object, which allows it to manipulate opaque micro objects, or large cells without causing photodamage. However, most existing laser-driven tools are limited to relatively simple tasks, such as moving and caging, and cannot carry out in-hand dexterous tasks. This is mainly because in-hand manipulation involves continuously coordinating multiple laser beams, micro-tools, and the object itself, which has high degrees of freedom (DoF) and poses up challenge for planner and controller design. This paper presents a new hierarchical formulation for the grasping and manipulation of micro objects using multiple laser-driven micro-tools. In hardware, multiple laser-driven tools are assembled to act as a robotic hand to carry out in-hand tasks (e.g., rotating); in software, a hierarchical scheme is developed to shrunken the action space and coordinate the motion of multiple tools, subject to both the parametric uncertainty in the tool and the unknown dynamic model of the object. Such a formulation provides potential for achieving robotic in-hand manipulation at a micro scale. The performance of the proposed system is validated in simulation studies under different scenarios.},
  author       = {Jia, Yongyi and Chen, Yu and Liu, Hao and Li, Xiu and Li, Xiang},
  bibtex_show  = {true},
  booktitle    = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  organization = {IEEE},
  pages        = {1047--1054},
  preview      = {jia2022hierarchical.png},
  title        = {Hierarchical Learning and Control for In-Hand Micromanipulation Using Multiple Laser-Driven Micro-Tools},
  year         = {2022},
}

@inproceedings{yan2022adaptive,
  abstract    = {Human-robot collaboration aims to extend human ability through cooperation with robots. This technology is currently helping people with physical disabilities, has transformed the manufacturing process of companies, improved surgical performance, and will likely revolutionize the daily lives of everyone in the future. Being able to enhance the performance of both sides, such that human-robot collaboration outperforms a single robot/human, remains an open issue. For safer and more effective collaboration, a new control scheme has been proposed for redundant robots in this paper, consisting of an adaptive vision-based control term in task space and an interactive control term in null space. Such a formulation allows the robot to autonomously carry out tasks in an unknown environment without prior calibration while also interacting with humans to deal with unforeseen changes (e.g., potential collision, temporary needs) under the redundant configuration. The decoupling between task space and null space helps to explore the collaboration safely and effectively without affecting the main task of the robot end-effector. The stability of the closed-loop system has been rigorously proved with Lyapunov methods, and both the convergence of the position error in task space and that of the damping model in null space are guaranteed. The experimental results of a robot manipulator guided with the technology of augmented reality (AR) are presented to illustrate the performance of the control scheme.},
  author      = {Yan, Xiangjie and Chen, Chen and Li, Xiang},
  bibtex_show = {true},
  booktitle   = {2022 International Conference on Robotics and Automation (ICRA)},
  code        = {https://github.com/yanseim/Vision-Based-Control},
  doi         = {10.1109/ICRA46639.2022.9812218},
  month       = {May},
  pages       = {2803-2809},
  preview     = {yan2022adaptive.jpeg},
  title       = {Adaptive Vision-Based Control of Redundant Robots with Null-Space Interaction for Human-Robot Collaboration},
  year        = {2022},
}

@inproceedings{yu2022shape,
  arxiv       = {2109.11091},
  author      = {Yu, Mingrui and Zhong, Hanzhong and Li, Xiang},
  bibtex_show = {true},
  booktitle   = {2022 International Conference on Robotics and Automation (ICRA)},
  code        = {https://github.com/Mingrui-Yu/shape_control_DLO},
  doi         = {10.1109/ICRA46639.2022.9812244},
  doi         = {10.1109/ICRA46639.2022.9812244},
  pages       = {1337-1343},
  preview     = {yu2022shape.gif},
  title       = {Shape Control of Deformable Linear Objects with Offline and Online Learning of Local Linear Deformation Models},
  website     = {https://mingrui-yu.github.io/shape_control_DLO/},
  year        = {2022},
}
